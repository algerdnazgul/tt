import logging
from datetime import datetime, timedelta

from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.operators.empty import EmptyOperator

from rubbles_dags.utils.constants import GCP_CONN_ID

logger = logging.getLogger(__name__)  # todo move?

def create_dag():
    dag = DAG(
        dag_id="update_dm_summary",
        start_date=datetime(2024, 5, 27),
        catchup=False,
        max_active_runs=1,
        default_args={
            "owner": "rubbles",
            "do_xcom_push": False,
            "retries": 0,
            "retry_delay": timedelta(minutes=1),
        },
        schedule_interval="30 10 * * *",
    )
    with dag:
        # Читаем SQL из файла (Airflow предпочитает явно читать на этапе генерации DAG)
        with open('/sql/dm_summary.sql', 'r') as f:
            sql_query = f.read()

        update_dm = BigQueryInsertJobOperator(
            task_id="update_dm_summary",
            configuration={
                "query": {
                    "query": sql_query,
                    "useLegacySql": False,
                }
            },
            gcp_conn_id=GCP_CONN_ID,
            execution_timeout=timedelta(minutes=5),
            location='asia-south1'  # если нужно
        )

        done = EmptyOperator(task_id="done")
        update_dm >> done

    return dag

globals()["update_dm_summary"] = create_dag()
